# Print selected features for verification
selected_features = tfidf_vectorizer.get_feature_names_out()
print(f"Selected features (total {len(selected_features)}):")
print(selected_features)

# Alternatively, visualize the top 20 features by their importance
import matplotlib.pyplot as plt

# Get the TF-IDF scores for each feature
feature_scores = np.sum(X_tfidf, axis=0).A1
feature_importance = pd.DataFrame({'Feature': selected_features, 'Importance': feature_scores})
feature_importance = feature_importance.sort_values(by='Importance', ascending=False).head(20)

# Plot the top 20 features
plt.figure(figsize=(12, 8))
plt.barh(feature_importance['Feature'], feature_importance['Importance'])
plt.xlabel('TF-IDF Score')
plt.ylabel('Feature')
plt.title('Top 20 TF-IDF Features')
plt.gca().invert_yaxis()
plt.show()

------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import ConfusionMatrixDisplay

# Evaluate the model
def evaluate_model(y_true, y_pred):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    roc_auc = roc_auc_score(y_true, y_pred)
    conf_matrix = confusion_matrix(y_true, y_pred)
    return accuracy, precision, recall, f1, conf_matrix

# Evaluation on test set
test_accuracy, test_precision, test_recall, test_f1, test_conf_matrix = evaluate_model(y_test_combined, final_test_preds)

# Print the test results
print(f'Test Accuracy: {test_accuracy:.2f}')
print(f'Test Precision: {test_precision:.2f}')
print(f'Test Recall: {test_recall:.2f}')
print(f'Test F1: {test_f1:.2f}')

# Plot the confusion matrix
fig, ax = plt.subplots(figsize=(8, 6))
ConfusionMatrixDisplay(test_conf_matrix, display_labels=['Actual Negative', 'Actual Positive']).plot(ax=ax)
plt.title('Confusion Matrix')
plt.show()

# Plotting the accuracy and other metrics
metrics = {
    'Model': ['Hybrid LSTM-SVM'],
    'Accuracy': [test_accuracy],
    'Precision': [test_precision],
    'Recall': [test_recall],
    'F1-Score': [test_f1],
}

metrics_df = pd.DataFrame(metrics)

# Plot Accuracy
plt.figure(figsize=(10, 6))
sns.barplot(x='Model', y='Accuracy', data=metrics_df, palette='viridis', edgecolor='black', linewidth=1.5)
plt.ylim(0.9, 1)
plt.title('Accuracy Comparison')
plt.show()

# Plot Precision, Recall, and F1-Score in one chart
metrics_df_melted = metrics_df.melt(id_vars='Model', value_vars=['Precision', 'Recall', 'F1-Score'], 
                                    var_name='Metric', value_name='Score')

plt.figure(figsize=(10, 6))
sns.barplot(x='Model', y='Score', hue='Metric', data=metrics_df_melted, palette='muted', edgecolor='black', linewidth=1.5)
plt.ylim(0.9, 1)
plt.title('Precision, Recall, and F1-Score Comparison')
plt.show()





